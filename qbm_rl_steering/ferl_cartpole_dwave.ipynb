{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "from qbm_rl_steering.agents.ferl import QBMQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hack_the_env(env: gym.Env) -> gym.Env:\n",
    "    # TODO: something is not right with the normalization. How do they do it properly in SB3 / gym?\n",
    "    #  The reason is that some of the bounds are +/- np.inf which my code does not like so much ...\n",
    "    # For now we hack the lows and highs of the default observation_space, but keep in mind. Might cause\n",
    "    # other problems as well ...\n",
    "    low = env.observation_space.low\n",
    "    high = env.observation_space.high\n",
    "\n",
    "    low[1] = -2.5\n",
    "    low[3] = -2.5\n",
    "    high[1] = 2.5\n",
    "    high[3] = 2.5\n",
    "\n",
    "    env.observation_space = gym.spaces.Box(low, high)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "def get_params() -> Tuple[Dict, Dict, Dict]:\n",
    "    \"\"\"Get parameters for RL, QBM, and annealing.\"\"\"\n",
    "    # RL settings\n",
    "    kwargs_rl = {\n",
    "        'learning_rate': (0.016, 0.016),  # (0.12, 0.12),\n",
    "        'small_gamma': 0.95,  # 0.90,\n",
    "        'exploration_epsilon': (0.95, 0.),\n",
    "        'exploration_fraction': 1.0,\n",
    "        'replay_batch_size': 16,  # 16, \n",
    "        'target_update_frequency': 1,\n",
    "        'soft_update_factor': 1.  # 0.6\n",
    "    }\n",
    "\n",
    "    # Graph config and quantum annealing settings\n",
    "    kwargs_anneal = {\n",
    "        'sampler_type': 'QPU',  # 'SQA',\n",
    "        'kwargs_qpu': {\n",
    "            'token': \"DEV-0e07ab13594ade21154e9244ec6c6069198a5260\",  # \"DEV-66ff199bc69a2ea5bb4223259859867c616de277\",\n",
    "            'solver': \"Advantage_system6.1\"},\n",
    "        'n_replicas': 1,\n",
    "        'n_meas_for_average': 1,\n",
    "        'n_annealing_steps': 150,  # 100,\n",
    "        'big_gamma': (1.2, 0.),  # (8.5, 0.),\n",
    "        'beta': 7.,  # 0.06,\n",
    "    }\n",
    "\n",
    "    # Q-function settings (QBM)\n",
    "    kwargs_qbm = {\n",
    "        \"n_columns_qbm\": 2,  # 1,\n",
    "        \"n_rows_qbm\": 2,  # 1\n",
    "    }\n",
    "\n",
    "    return kwargs_rl, kwargs_anneal, kwargs_qbm\n",
    "\n",
    "\n",
    "def evaluate(agent: QBMQ, n_evals: int = 10) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Evaluate agent for n_evals episodes.\n",
    "    :param agent: RL agent using QBM\n",
    "    :param n_evals: Number of episodes to evaluate\n",
    "    :return: stats on performance\n",
    "    \"\"\"\n",
    "    n_steps_list = []\n",
    "    for _ in range(n_evals):\n",
    "        n_steps = run_episode(agent)\n",
    "        n_steps_list.append(n_steps)\n",
    "\n",
    "    n_steps_list = np.array(n_steps_list)\n",
    "    avg_, std_ = np.mean(n_steps_list), np.std(n_steps_list)\n",
    "    max_, min_ = np.max(n_steps_list), np.min(n_steps_list)\n",
    "    return avg_, std_, max_, min_\n",
    "\n",
    "\n",
    "def run_episode(agent: QBMQ, with_render: bool = False) -> int:\n",
    "    \"\"\"\n",
    "    Run one episode of the agent.\n",
    "    :param agent: RL agent using QBM.\n",
    "    :param with_render: flag whether to render the environment.\n",
    "    :return: total number of steps needed.\n",
    "    \"\"\"\n",
    "    env = agent.env\n",
    "    obs = env.reset()\n",
    "    total_steps = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        if total_steps % 10 == 0:\n",
    "            print(f'Running episode, step {total_steps}')\n",
    "        act, _ = agent.predict(obs, deterministic=True)\n",
    "        # act = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(act)\n",
    "        if with_render:\n",
    "            env.render()\n",
    "            time.sleep(0.01)\n",
    "        total_steps += 1\n",
    "\n",
    "    if with_render:\n",
    "        env.close()\n",
    "\n",
    "    return total_steps\n",
    "\n",
    "\n",
    "def full_run(n_training_steps: int = 20) -> Tuple[QBMQ, pd.DataFrame]:\n",
    "    \"\"\"Run entire training with evaluations before and after.\"\"\"\n",
    "    kwargs_rl, kwargs_anneal, kwargs_qbm = get_params()\n",
    "\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    env = hack_the_env(env)\n",
    "    kwargs_anneal.update({\"sampler_type\": \"SQA\"})\n",
    "    agent = QBMQ(env=env, **kwargs_anneal, **kwargs_rl, **kwargs_qbm)\n",
    "    \n",
    "\n",
    "    # Evaluate agent first\n",
    "    print(f'BEFORE TRAINING')\n",
    "    avg_i, std_i, max_i, min_i = evaluate(agent, n_evals=1)\n",
    "    \n",
    "    # Save weights\n",
    "    w_hh = agent.q_function.w_hh.copy()\n",
    "    w_vh = agent.q_function.w_vh.copy()\n",
    "    w_hh_t = agent.q_function_target.w_hh.copy()\n",
    "    w_vh_t = agent.q_function_target.w_vh.copy()\n",
    " \n",
    "    # Train the agent\n",
    "    print(f'TRAIN AGENT')\n",
    "    kwargs_anneal.update({\"sampler_type\": \"QPU\"})\n",
    "    agent = QBMQ(env=env, **kwargs_anneal, **kwargs_rl, **kwargs_qbm)\n",
    "    agent.q_function.w_hh = w_hh\n",
    "    agent.q_function.w_vh = w_hh\n",
    "    agent.q_function_target.w_hh = w_hh_t\n",
    "    agent.q_function_target.w_vh = w_hh_t\n",
    "    agent.learn(total_timesteps=n_training_steps)\n",
    "\n",
    "    # Re-evaluate agent again\n",
    "    print(f'AFTER TRAINING')\n",
    "    # Save weights, trained agent\n",
    "    w_hh = agent.q_function.w_hh.copy()\n",
    "    w_vh = agent.q_function.w_vh.copy()\n",
    "    w_hh_t = agent.q_function_target.w_hh.copy()\n",
    "    w_vh_t = agent.q_function_target.w_vh.copy()\n",
    "    \n",
    "    kwargs_anneal.update({\"sampler_type\": \"SQA\"})\n",
    "    agent = QBMQ(env=env, **kwargs_anneal, **kwargs_rl, **kwargs_qbm)\n",
    "    \n",
    "    agent.q_function.w_hh = w_hh\n",
    "    agent.q_function.w_vh = w_hh\n",
    "    agent.q_function_target.w_hh = w_hh_t\n",
    "    agent.q_function_target.w_vh = w_hh_t\n",
    "    \n",
    "    avg_f, std_f, max_f, min_f = evaluate(agent, n_evals=1)\n",
    "    # print(f'Avg +/- std: {avg_f:.1f} +/- {std_f:.1f}')\n",
    "    # print(f'Min, max: {min_f:.0f}, {max_f:.0f}')\n",
    "\n",
    "    res_df = pd.DataFrame([{\n",
    "        \"avg_i\": avg_i, \"avg_f\": avg_f, \"std_i\": std_i, \"std_f\": std_f,\n",
    "        \"max_i\": max_i, \"max_f\": max_f, \"min_i\": min_i, \"min_f\": min_f\n",
    "    }])\n",
    "\n",
    "    return agent, res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETTING PROPER QPU AS SAMPLER\n",
      "QPU Advantage_system6.1 was selected.\n",
      " ! Warning: big_gammas are 'virtual'. We don't know the actual values ... \n",
      "SETTING PROPER QPU AS SAMPLER\n",
      "QPU Advantage_system6.1 was selected.\n",
      " ! Warning: big_gammas are 'virtual'. We don't know the actual values ... \n",
      "BEFORE TRAINING\n",
      "Running episode, step 0\n",
      "Running episode, step 10\n",
      "Running episode, step 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [04:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "embedding cancelled by keyboard interrupt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-eff62b18b01a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_training_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b15c3420409c>\u001b[0m in \u001b[0;36mfull_run\u001b[0;34m(n_training_steps)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# Evaluate agent first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'BEFORE TRAINING'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mavg_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# Train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b15c3420409c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(agent, n_evals)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mn_steps_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_evals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mn_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mn_steps_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b15c3420409c>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(agent, with_render)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Running episode, step {total_steps}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;31m# act = env.action_space.sample()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp3/qbm-rl-steering/qbm_rl_steering/agents/ferl.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state, deterministic)\u001b[0m\n\u001b[1;32m    301\u001b[0m         interface.\"\"\"\n\u001b[1;32m    302\u001b[0m         action, q_value, samples, visible_nodes = (\n\u001b[0;32m--> 303\u001b[0;31m             self.follow_policy(state=state, epsilon=0.))\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp3/qbm-rl-steering/qbm_rl_steering/agents/ferl.py\u001b[0m in \u001b[0;36mfollow_policy\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpossible_actions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 q_value, spin_configurations, visible_nodes = (\n\u001b[0;32m--> 278\u001b[0;31m                     self.q_function_target.calculate_q_value(\n\u001b[0m\u001b[1;32m    279\u001b[0m                         state=state, action=action))\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp3/qbm-rl-steering/qbm_rl_steering/core/qbm.py\u001b[0m in \u001b[0;36mcalculate_q_value\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;31m# Run sampling process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         spin_configurations = self.sampler.sample(\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0mqubo_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqubo_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0mn_meas_for_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_meas_for_average\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp3/qbm-rl-steering/qbm_rl_steering/samplers/qpu_annealer.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, qubo_dict, n_meas_for_average, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mnum_reads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_meas_for_average\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_replicas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         spin_configurations = list(self.annealer.sample_qubo(\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqubo_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_reads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_reads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# beta=self.beta_final,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/dimod/core/sampler.py\u001b[0m in \u001b[0;36msample_qubo\u001b[0;34m(self, Q, **parameters)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"\n\u001b[1;32m    290\u001b[0m         \u001b[0mbqm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryQuadraticModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_qubo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbqm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mremove_unknown_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/dwave/system/composites/embedding.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, bqm, chain_strength, chain_break_method, chain_break_fraction, embedding_parameters, return_embedding, warnings, **parameters)\u001b[0m\n\u001b[1;32m    233\u001b[0m                                         if key not in embedding_parameters)\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         embedding = self.find_embedding(source_edgelist, target_edgelist,\n\u001b[0m\u001b[1;32m    236\u001b[0m                                         **embedding_parameters)\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/minorminer/minorminer.py\u001b[0m in \u001b[0;36mfind_embedding\u001b[0;34m(S, T, max_no_improvement, random_seed, timeout, max_beta, tries, inner_rounds, chainlength_patience, max_fill, threads, return_overlap, skip_initialization, verbose, interactive, initial_chains, fixed_chains, restrict_chains, suspend_chains)\u001b[0m\n\u001b[1;32m     39\u001b[0m                    \u001b[0msuspend_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                    ):\n\u001b[0;32m---> 41\u001b[0;31m     return __find_embedding(S, T,\n\u001b[0m\u001b[1;32m     42\u001b[0m                             \u001b[0mmax_no_improvement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_no_improvement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                             \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mminorminer/_minorminer.pyx\u001b[0m in \u001b[0;36mminorminer._minorminer.find_embedding\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: embedding cancelled by keyboard interrupt"
     ]
    }
   ],
   "source": [
    "n_training_steps = 50\n",
    "\n",
    "n_runs = 1\n",
    "results_df = pd.DataFrame()\n",
    "for i in trange(n_runs):\n",
    "    agent, res = full_run(n_training_steps)\n",
    "    results_df = pd.concat([results_df, res])\n",
    "results_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Plot results\n",
    "ax = results_df.plot(y=[\"avg_i\", \"avg_f\"], marker='x',\n",
    "                     label=[\"Before training\", \"After training\"])\n",
    "ax.set_ylabel('Cumulative reward')\n",
    "ax.set_xlabel('Run nb.')\n",
    "\n",
    "results_df.to_pickle(\"res_run2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETTING PROPER QPU AS SAMPLER\n",
      "QPU Advantage_system6.1 was selected.\n",
      " ! Warning: big_gammas are 'virtual'. We don't know the actual values ... \n",
      "SETTING PROPER QPU AS SAMPLER\n",
      "QPU Advantage_system6.1 was selected.\n",
      " ! Warning: big_gammas are 'virtual'. We don't know the actual values ... \n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env = hack_the_env(env)\n",
    "kwargs_rl, kwargs_anneal, kwargs_qbm = get_params()\n",
    "agent = QBMQ(env=env, **kwargs_anneal, **kwargs_rl, **kwargs_qbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hh = agent.q_function.w_hh.copy()\n",
    "w_vh = agent.q_function.w_vh.copy()\n",
    "\n",
    "w_hh_t = agent.q_function_target.w_hh.copy()\n",
    "w_vh_t = agent.q_function_target.w_vh.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
